{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa981e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScrapePubMed:\n",
    "    \n",
    "    def initialize_variables(self):\n",
    "        self.email = None\n",
    "        self.search_term = None\n",
    "        self.no_space_search_term = None\n",
    "        self.retmax_search = None\n",
    "        self.retmax_fetch = None\n",
    "        self.search_term_count = None\n",
    "        self.list_of_ids = []\n",
    "        self.filename = None\n",
    "        \n",
    "    def set_email(self):\n",
    "        self.email = input(\"Type in your email: \")\n",
    "        \n",
    "    def set_search_term(self):\n",
    "        self.search_term = input(\"Type in your search term: \")\n",
    "        self.no_space_search_term = self.search_term.replace(\" \",\"\")\n",
    "        \n",
    "    def set_retmax_search(self):\n",
    "        while type(self.retmax_search) is not int or self.retmax_search <= 0 or self.retmax_search >= 10001:\n",
    "            try:\n",
    "                self.retmax_search = int(input(\"Between 1-10000, how many articles to search for in each batch? (<10k): \"))\n",
    "            except ValueError:\n",
    "                print(\"Please type in a number from 1-10000: \")\n",
    "                \n",
    "    def set_retmax_fetch(self):\n",
    "        while type(self.retmax_fetch) is not int or self.retmax_fetch <= 0 or self.retmax_fetch >= 10001:\n",
    "            try:\n",
    "                self.retmax_fetch = int(input(\"Between 1-10000, how many articles to fetch in each batch? (<10k): \"))\n",
    "            except ValueError:\n",
    "                print(\"Please type in a number from 1-10000: \")\n",
    "                \n",
    "    def get_search_term_count(self):\n",
    "        Entrez.email = self.email\n",
    "\n",
    "        handle = Entrez.esearch(db='pubmed', retmode='xml', term=self.search_term)\n",
    "        # api_key = API_KEY\n",
    "        record = Entrez.read(handle)\n",
    "        self.search_term_count = int(record[\"Count\"])\n",
    "        \n",
    "    def get_list_of_ids(self):\n",
    "        Entrez.email = self.email\n",
    "        retstart = 0\n",
    "        count = self.search_term_count\n",
    "\n",
    "        while count > 0:\n",
    "            handle = Entrez.esearch(db=\"pubmed\", retmode = 'xml', term = self.search_term, retmax = self.retmax_search, retstart = retstart)\n",
    "            # api_key = API_KEY\n",
    "            record = Entrez.read(handle)\n",
    "            idlist = record['IdList']\n",
    "            self.list_of_ids.append(idlist)\n",
    "\n",
    "            count -= int(len(idlist))\n",
    "            retstart += int(len(idlist))\n",
    "            \n",
    "    def download_articles(self, filepath):\n",
    "        total_time = time.time()\n",
    "\n",
    "        Entrez.email = self.email\n",
    "\n",
    "        next_article = 0\n",
    "        next_batch = 0\n",
    "\n",
    "        self.filename = f\"{filepath}/{self.no_space_search_term}.csv\"\n",
    "\n",
    "        while self.search_term_count > 0:\n",
    "            while int(len(self.list_of_ids[next_batch])) > 0:\n",
    "\n",
    "                handle = Entrez.efetch(db = 'pubmed', id=self.list_of_ids[next_batch], retmode = 'xml', retmax = self.retmax_fetch)\n",
    "                # api_key = API_KEY\n",
    "                results = Entrez.read(handle)\n",
    "\n",
    "                json_format = json.dumps(results)\n",
    "                python_dictionary = json.loads(json_format)\n",
    "\n",
    "                for document in python_dictionary['PubmedArticle']:\n",
    "                    xml = dicttoxml(document)\n",
    "                    tree = ET.ElementTree(ET.fromstring(xml)) \n",
    "\n",
    "                    for entry in tree.findall('MedlineCitation/Article/AuthorList/item'):\n",
    "\n",
    "                        first_name = []\n",
    "                        for f_name in entry.findall('ForeName'):\n",
    "                            first_name.append(f_name.text)\n",
    "\n",
    "                        last_name = []\n",
    "                        for l_name in entry.findall('LastName'):\n",
    "                            last_name.append(l_name.text)\n",
    "\n",
    "                        affiliation = []\n",
    "                        for aff in entry.findall('AffiliationInfo/item/Affiliation'):\n",
    "                            affiliation.append(aff.text)\n",
    "\n",
    "                        node = tree.find('MedlineCitation/Article/ArticleTitle')\n",
    "                        if node is not None:\n",
    "                            title = node.text\n",
    "                        else:\n",
    "                            title = None\n",
    "\n",
    "                        node = tree.find('MedlineCitation/Article/ArticleDate/item/Year')\n",
    "                        if node is not None:\n",
    "                            publication = node.text\n",
    "                        else:\n",
    "                            publication = None\n",
    "\n",
    "                        node = tree.find('MedlineCitation/PMID')\n",
    "                        if node is not None:\n",
    "                            pmid = node.text\n",
    "                        else:\n",
    "                            pmid = None\n",
    "\n",
    "                        dataframe = pd.DataFrame()\n",
    "                        dataframe = dataframe.append([first_name, last_name, affiliation]).transpose()\n",
    "                        dataframe.columns = ['FirstName', 'LastName', 'Affiliation']\n",
    "                        dataframe['Title'] = title\n",
    "                        dataframe['Publication'] = publication\n",
    "                        dataframe['PMID'] = pmid\n",
    "\n",
    "                        dataframe.to_csv(self.filename, mode='a')\n",
    "\n",
    "                self.list_of_ids[next_batch] = self.list_of_ids[next_batch][self.retmax_fetch:]\n",
    "                self.search_term_count -= self.retmax_fetch\n",
    "\n",
    "                print(f'{self.search_term_count} articles to go')\n",
    "\n",
    "\n",
    "                if int(len(self.list_of_ids)) <= 0:\n",
    "                    break\n",
    "\n",
    "            next_batch += 1 \n",
    "\n",
    "        end_total_time = time.time()\n",
    "        finish = round(end_total_time-total_time)\n",
    "\n",
    "        print('script complete')\n",
    "        print(f'script took {finish} seconds to complete')\n",
    "\n",
    "    def __init__(self):\n",
    "        self.initialize_variables()\n",
    "        self.set_email()\n",
    "        self.set_search_term()\n",
    "        self.set_retmax_search()\n",
    "        self.set_retmax_fetch()\n",
    "        self.get_search_term_count()\n",
    "        self.get_list_of_ids()\n",
    "        self.download_articles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed53fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanData:\n",
    "    \n",
    "    def initialize_variables(self):\n",
    "        self.df = pd.DataFrame()\n",
    "        self.filepath = None\n",
    "        self.series = None\n",
    "        self.info_df = None\n",
    "        self.final_df = None\n",
    "        self.return_str = None\n",
    "        self.countries = [\n",
    "        'Afghanistan',\n",
    "        'Albania',\n",
    "        'Algeria',\n",
    "        'Andorra',\n",
    "        'Angola',\n",
    "        'Antigua & Deps',\n",
    "        'Argentina',\n",
    "        'Armenia',\n",
    "        'Australia',\n",
    "        'Austria',\n",
    "        'Azerbaijan',\n",
    "        'Bahamas',\n",
    "        'Bahrain',\n",
    "        'Bangladesh',\n",
    "        'Barbados',\n",
    "        'Belarus',\n",
    "        'Belgium',\n",
    "        'Belize',\n",
    "        'Benin',\n",
    "        'Bhutan',\n",
    "        'Bolivia',\n",
    "        'Bosnia & Herzegovina',\n",
    "        'Botswana',\n",
    "        'Brazil',\n",
    "        'Brunei',\n",
    "        'Bulgaria',\n",
    "        'Burkina',\n",
    "        'Burundi',\n",
    "        'Cambodia',\n",
    "        'Cameroon',\n",
    "        'Canada',\n",
    "        'Cape Verde',\n",
    "        'Central African Rep',\n",
    "        'Chad',\n",
    "        'Chile',\n",
    "        'China',\n",
    "        'Colombia',\n",
    "        'Comoros',\n",
    "        'Congo',\n",
    "        'Congo {Democratic Rep}',\n",
    "        'Costa Rica',\n",
    "        'Croatia',\n",
    "        'Cuba',\n",
    "        'Cyprus',\n",
    "        'Czech Republic',\n",
    "        'Denmark',\n",
    "        'Djibouti',\n",
    "        'Dominica',\n",
    "        'Dominican Republic',\n",
    "        'East Timor',\n",
    "        'Ecuador',\n",
    "        'Egypt',\n",
    "        'El Salvador',\n",
    "        'Equatorial Guinea',\n",
    "        'Eritrea',\n",
    "        'Estonia',\n",
    "        'Ethiopia',\n",
    "        'Fiji',\n",
    "        'Finland',\n",
    "        'France',\n",
    "        'Gabon',\n",
    "        'Gambia',\n",
    "        'Georgia',\n",
    "        'Germany',\n",
    "        'Ghana',\n",
    "        'Greece',\n",
    "        'Grenada',\n",
    "        'Guatemala',\n",
    "        'Guinea',\n",
    "        'Guinea-Bissau',\n",
    "        'Guyana',\n",
    "        'Haiti',\n",
    "        'Honduras',\n",
    "        'Hungary',\n",
    "        'Iceland',\n",
    "        'India',\n",
    "        'Indonesia',\n",
    "        'Iran',\n",
    "        'Iraq',\n",
    "        'Ireland',\n",
    "        'Israel',\n",
    "        'Italy',\n",
    "        'Ivory Coast',\n",
    "        'Jamaica',\n",
    "        'Japan',\n",
    "        'Jordan',\n",
    "        'Kazakhstan',\n",
    "        'Kenya',\n",
    "        'Kiribati',\n",
    "        'Korea North',\n",
    "        'Korea South',\n",
    "        'Kosovo',\n",
    "        'Kuwait',\n",
    "        'Kyrgyzstan',\n",
    "        'Laos',\n",
    "        'Latvia',\n",
    "        'Lebanon',\n",
    "        'Lesotho',\n",
    "        'Liberia',\n",
    "        'Libya',\n",
    "        'Liechtenstein',\n",
    "        'Lithuania',\n",
    "        'Luxembourg',\n",
    "        'Macedonia',\n",
    "        'Madagascar',\n",
    "        'Malawi',\n",
    "        'Malaysia',\n",
    "        'Maldives',\n",
    "        'Mali',\n",
    "        'Malta',\n",
    "        'Marshall Islands',\n",
    "        'Mauritania',\n",
    "        'Mauritius',\n",
    "        'Mexico',\n",
    "        'Micronesia',\n",
    "        'Moldova',\n",
    "        'Monaco',\n",
    "        'Mongolia',\n",
    "        'Montenegro',\n",
    "        'Morocco',\n",
    "        'Mozambique',\n",
    "        'Myanmar, {Burma}',\n",
    "        'Namibia',\n",
    "        'Nauru',\n",
    "        'Nepal',\n",
    "        'Netherlands',\n",
    "        'New Zealand',\n",
    "        'Nicaragua',\n",
    "        'Niger',\n",
    "        'Nigeria',\n",
    "        'Norway',\n",
    "        'Oman',\n",
    "        'Pakistan',\n",
    "        'Palau',\n",
    "        'Panama',\n",
    "        'Papua New Guinea',\n",
    "        'Paraguay',\n",
    "        'Peru',\n",
    "        'Philippines',\n",
    "        'Poland',\n",
    "        'Portugal',\n",
    "        'PR'\n",
    "        'Qatar',\n",
    "        'Romania',\n",
    "        'Russian Federation',\n",
    "        'Rwanda',\n",
    "        'St Kitts & Nevis',\n",
    "        'St Lucia',\n",
    "        'Saint Vincent & the Grenadines',\n",
    "        'Samoa',\n",
    "        'San Marino',\n",
    "        'Sao Tome & Principe',\n",
    "        'Saudi Arabia',\n",
    "        'Senegal',\n",
    "        'Serbia',\n",
    "        'Seychelles',\n",
    "        'Sierra Leone',\n",
    "        'Singapore',\n",
    "        'Slovakia',\n",
    "        'Slovenia',\n",
    "        'Solomon Islands',\n",
    "        'Somalia',\n",
    "        'South Africa',\n",
    "        'South Sudan',\n",
    "        'Spain',\n",
    "        'Sri Lanka',\n",
    "        'Sudan',\n",
    "        'Suriname',\n",
    "        'Swaziland',\n",
    "        'Sweden',\n",
    "        'Switzerland',\n",
    "        'Syria',\n",
    "        'Taiwan',\n",
    "        'Tajikistan',\n",
    "        'Tanzania',\n",
    "        'Thailand',\n",
    "        'Togo',\n",
    "        'Tonga',\n",
    "        'Trinidad & Tobago',\n",
    "        'Tunisia',\n",
    "        'Turkey',\n",
    "        'Turkmenistan',\n",
    "        'Tuvalu',\n",
    "        'Uganda',\n",
    "        'Ukraine',\n",
    "        'United Arab Emirates',\n",
    "        'UK',\n",
    "        'United Kingdom',\n",
    "        'United States',\n",
    "        'USA',\n",
    "        'Uruguay',\n",
    "        'Uzbekistan',\n",
    "        'Vanuatu',\n",
    "        'Vatican City',\n",
    "        'Venezuela',\n",
    "        'Vietnam',\n",
    "        'Yemen',\n",
    "        'Zambia',\n",
    "        'Zimbabwe'\n",
    "        ]\n",
    "        self.pattern = '|'.join(self.countries)\n",
    "        \n",
    "    def csv_to_dataframe(self):\n",
    "        self.filepath = input(\"Where is your file?: \")\n",
    "        self.df = pd.read_csv(self.filepath)\n",
    "        \n",
    "    def drop_unnecessary_column_row(self):\n",
    "        self.df = self.df.drop('Unnamed: 0', axis=1) \n",
    "        self.df = self.df[~self.df['FirstName'].isin(['FirstName'])]\n",
    "        \n",
    "    def clean_affiliation_column(self):\n",
    "        self.df = self.df.dropna(subset=['Affiliation'])\n",
    "           \n",
    "    def make_email_column(self):\n",
    "        # make new column of emails\n",
    "        self.df['Email'] = self.df['Affiliation'].str.findall(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+')\n",
    "        # remove all rows where email is an empty list\n",
    "        self.df = self.df[self.df['Email'].map(lambda x: len(x)) > 0]\n",
    "        # for any row that has multiple emails, make a new row for each email\n",
    "        self.df = self.df.explode('Email')\n",
    "        # remove any periods that are at the end of emails\n",
    "        self.df['Email'] = self.df['Email'].str.rstrip('.')   \n",
    "        \n",
    "    def drop_email_from_affiliation(self):\n",
    "        # remove the email from column 'Affiliation'\n",
    "        self.df['Affiliation'] = self.df['Affiliation'].str.replace(r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', '')\n",
    "        \n",
    "    # search through a string and if a word in the string is in a word list(str) then return the word from the string\n",
    "    def pattern_searcher(self, search_str:str, search_list:str):\n",
    "        search_obj = re.search(search_list, search_str)\n",
    "        if search_obj:\n",
    "            self.return_str = search_str[search_obj.start(): search_obj.end()]\n",
    "        else:\n",
    "            self.return_str = 'NaN'\n",
    "            \n",
    "        return self.return_str\n",
    "            \n",
    "    def use_pattern_searcher(self):\n",
    "        self.df['Country'] = self.df['Affiliation'].apply(lambda x: self.pattern_searcher(search_str=x, search_list=self.pattern))\n",
    "        self.df['Country'] = self.df['Country'].replace('NaN', np.NaN)\n",
    "\n",
    "    def combine_affiliation_per_author(self):\n",
    "        self.df = self.df.groupby(['PMID', 'FirstName','LastName', 'Title', 'Publication', 'Email', 'Country'])['Affiliation'].apply(', '.join).reset_index()\n",
    "        \n",
    "    def separate_affiliation_entries(self):\n",
    "        # split string in 'Affiliation' column by each comma into new columns\n",
    "        self.df = pd.concat([self.df, self.df['Affiliation'].str.split(',', expand=True)], axis=1)\n",
    "        \n",
    "    def combine_by_attribute(self, dataframe, series, subject):\n",
    "        # -10 to not count columns that are not integers     \n",
    "        len_of_columns = int(len(self.df.columns)) - 10\n",
    "        counter = 1\n",
    "        while counter < len_of_columns:\n",
    "            series = series.append(self.df[counter].loc[self.df[counter].str.contains('|'.join(subject), case=False, na=False)])\n",
    "            counter += 1\n",
    "            \n",
    "        return series\n",
    "\n",
    "    def create_info_column(self, info_words, column_name):    \n",
    "        info = info_words\n",
    "        info_series = self.df[0].loc[self.df[0].str.contains('|'.join(info), case=False, na=False)]\n",
    "        self.info_df = self.combine_by_attribute(self.df, info_series, info)\n",
    "        self.info_df = pd.DataFrame(self.info_df)\n",
    "        self.info_df = self.info_df.rename(columns={0: f'{column_name}'})\n",
    "        \n",
    "        return self.info_df\n",
    "        \n",
    "    def comp_dept(self):\n",
    "        companies = ['University|College|Institute|School|Academy|Hospital|Clinic|Medicine']\n",
    "        company_dataframe = self.create_info_column(companies, self.df, 'Company')\n",
    "  \n",
    "        departments = ['Department', 'Dept.', 'Division']\n",
    "        department_dataframe = self.create_info_column(departments, self.df, 'Department')\n",
    "\n",
    "        self.df = self.df.join(company_dataframe, how='outer', rsuffix='University')\n",
    "        self.df = self.df.join(department_dataframe, how='outer', rsuffix='Department')\n",
    "        \n",
    "    def replace_text(self):\n",
    "        # replace 'Electronic address:' in all text with nothing\n",
    "        self.df['Department'] = self.df['Department'].str.replace('Electronic address:', '')\n",
    "        self.df['Company'] = self.df['Company'].str.replace('Electronic address:', '')\n",
    "        # replace any countries in all text with nothing\n",
    "        self.df['Department'] = self.df['Department'].str.replace('|'.join(self.countries), '')\n",
    "        # replace text 'PR: ;' in all text with nothing\n",
    "        self.df['Department'] = self.df['Department'].str.replace('PR ;', '')\n",
    "        \n",
    "    def remove_clutter(self):\n",
    "        # drop unnecessary columns\n",
    "        column_int_count = 0\n",
    "        for i in range(len(self.df.columns)-10):\n",
    "            self.df.drop([column_int_count], axis=1, inplace=True)\n",
    "            column_int_count += 1\n",
    "        # drop rows where columns 'FirstName'/'LastName' is empty\n",
    "        self.df = self.df.dropna(subset=['FirstName', 'LastName'])\n",
    "        # remove duplicate indexes\n",
    "        self.df = self.df[~self.df.index.duplicated(keep='first')]\n",
    "        # replace empty lists with NaN\n",
    "        self.df = self.df.mask(self.df.applymap(str).eq('[]'))\n",
    "        \n",
    "    def drop_email_duplicates(self):\n",
    "        self.df = self.df.drop_duplicates(subset='Email', keep=\"first\")\n",
    "        \n",
    "    def make_final_dataframe(self):\n",
    "        self.final_df = pd.DataFrame(columns=['Salutation', 'First Name', 'Last Name', 'Phone', 'Email', 'Position', 'Company', 'Department', 'Domain', 'Comment', 'Tags', 'Source', 'Country', 'State', 'City', 'Address', 'Zipcode'])\n",
    "        self.final_df['First Name'] = self.df['FirstName']\n",
    "        self.final_df['Last Name'] = self.df['LastName']\n",
    "        self.final_df['Field1'] = self.df['Title']\n",
    "        self.final_df['Field2'] = self.df['Publication']\n",
    "        self.final_df['Field3'] = self.df['PMID']\n",
    "        self.final_df['Email'] = self.df['Email']\n",
    "        self.final_df['Country'] = self.df['Country']\n",
    "        self.final_df['Department'] = self.df['Department']\n",
    "        self.final_df['Company'] = self.df['Company']\n",
    "        self.final_df['Comment'] = self.df['Affiliation']\n",
    "        \n",
    "    def invert_index(self):\n",
    "        self.final_df.reindex(index=self.final_df.index[::-1])\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.initialize_variables()\n",
    "        self.csv_to_dataframe()\n",
    "        self.drop_unnecessary_column_row()\n",
    "        self.clean_affiliation_column()\n",
    "        self.make_email_column()\n",
    "        self.drop_email_from_affiliation()\n",
    "        self.use_pattern_searcher()\n",
    "        self.combine_affiliation_per_author()\n",
    "        self.separate_affiliation_entries()\n",
    "        self.comp_dept()\n",
    "        self.replace_text()\n",
    "        self.remove_clutter()\n",
    "        self.drop_email_duplicates()\n",
    "        self.make_final_dataframe()\n",
    "        self.invert_index()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pubmed",
   "language": "python",
   "name": "pubmed"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
